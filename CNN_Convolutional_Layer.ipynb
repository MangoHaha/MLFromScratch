{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Convolutional_Layer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MangoHaha/MLFromScratch/blob/master/CNN_Convolutional_Layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNSFjvYUn7-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_forward(X, F):\n",
        "  '''\n",
        "  Assuming stride = 1, no padding\n",
        "  The forward computation for conv layer\n",
        "  X input for the current layer from previous layer\n",
        "  F filter, assuming size 1\n",
        "  '''\n",
        "  \n",
        "  H, W = np.shape(X) #highth and width of dataset\n",
        "  fH, fW = np.shape(F) #high and width of filter\n",
        "  \n",
        "  #init the output \n",
        "  oH = H - fH +1;\n",
        "  oW = W - fW +1;\n",
        "  O = np.zeros(oH, oW)\n",
        "  \n",
        "  for h in range(H):\n",
        "    for w in range(W):\n",
        "      O[h,w] = np.sum(X[h:h+fH, w:w+fW]*F)\n",
        "\n",
        "  # Saving information in 'cache' for backprop\n",
        "  cache = (X, W)\n",
        "  return O"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6E28SCpusdv",
        "colab_type": "text"
      },
      "source": [
        "in practice we usually only compute the gradient for the parameters (e.g. W,b) so that we can use it to perform a parameter update. However, as we will see later in the class the gradient on xi can still be useful sometimes, for example for purposes of visualization and interpreting what the Neural Network might be doing.\n",
        "Keep in mind what the derivatives tell you: They indicate the rate of change of a function with respect to that variable surrounding an infinitesimally small region near a particular point:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXVIgP06utR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_backward(dH, cache):\n",
        "    '''\n",
        "    The backward computation for a convolution function\n",
        "    \n",
        "    Arguments:\n",
        "    dH -- gradient of the cost with respect to output of the conv layer (H), numpy array of shape (n_H, n_W) assuming channels = 1\n",
        "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
        "    \n",
        "    Returns:\n",
        "    dX -- gradient of the cost with respect to input of the conv layer (X), numpy array of shape (n_H_prev, n_W_prev) assuming channels = 1\n",
        "    dW -- gradient of the cost with respect to the weights of the conv layer (W), numpy array of shape (f,f) assuming single filter\n",
        "    '''\n",
        "    \n",
        "    # Retrieving information from the \"cache\"\n",
        "    (X, W) = cache\n",
        "    \n",
        "    # Retrieving dimensions from X's shape\n",
        "    (n_H_prev, n_W_prev) = X.shape\n",
        "    \n",
        "    # Retrieving dimensions from W's shape\n",
        "    (f, f) = W.shape\n",
        "    \n",
        "    # Retrieving dimensions from dH's shape\n",
        "    (n_H, n_W) = dH.shape\n",
        "    \n",
        "    # Initializing dX, dW with the correct shapes\n",
        "    dX = np.zeros(X.shape)\n",
        "    dW = np.zeros(W.shape)\n",
        "    \n",
        "    # Looping over vertical(h) and horizontal(w) axis of the output\n",
        "    for h in range(n_H):\n",
        "        for w in range(n_W):\n",
        "            dX[h:h+f, w:w+f] += W * dH(h,w)\n",
        "            dW += X[h:h+f, w:w+f] * dH(h,w)\n",
        "    \n",
        "    return dX, dW"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}