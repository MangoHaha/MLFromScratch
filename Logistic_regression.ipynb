{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MangoHaha/MLFromScratch/blob/master/Logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88FIOzNKYPed",
        "colab_type": "code",
        "outputId": "7205e57d-013f-4e9f-e946-9433577cc081",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "!pip install sklearn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import sys"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.21.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.16.4)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDnzHK9bYlxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shuffle_data(X, y, seed=None):\n",
        "  #concatenate x and y for random shuffle\n",
        "  X_y = np.concatenate((X, y.reshape(len(y), 1)), axis=1)\n",
        "  if seed:\n",
        "    np.ramdom.seed(seed)\n",
        "  np.random.shuffle(X_y)\n",
        "  X = X_y[:, :-1] #every col except last\n",
        "  y = X-y[:, -1]  #every last col\n",
        "  return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_I_3XPUYWb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_split(X, y, test_size=0.5, seed=None):\n",
        "  if seed:\n",
        "    X, y = shuffle_data(X, y, seed)\n",
        "  split = len(y) - int(len(y)//(1/test_size))\n",
        "  train_X, test_X = X[:split], X[split:]\n",
        "  train_y, test_y = y[:split], y[split:]\n",
        "  return train_X, test_X, train_y, test_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_RwSE1ivLSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_square_error(y, label):\n",
        "  return np.mean(np.power(y-label, 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teGd4dyJbH0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def standardize(X):\n",
        "  mean = X.mean(axis=0)\n",
        "  std = X.std(axis=0)\n",
        "  X_std = X\n",
        "  for col in range(np.shape(X)[1]):\n",
        "    if(std[col]):\n",
        "      X_std[:, col] = (X_std[:, col] - mean[col])/std[col]\n",
        "  return X_std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqWxcsXN2LNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(X, axis=-1, order=2):\n",
        "  l2=np.atleast_1d(np.linalg.norm(X, order, axis))\n",
        "  l2[l2==0] = 1\n",
        "  return X/np.expand_dims(l2, axis)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnesE5Cd17cn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(X):\n",
        "  return 1/(1 + np.exp(-X))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6zgz44o3K7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy_score(y, pre_y):\n",
        "  score=0\n",
        "  for i in range(len(y)):\n",
        "    if y[i] == pre_y[i]:\n",
        "      score = score + 1\n",
        "  return score/len(y)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgo0-3rI4nU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Return the variance of the features in dataset X\n",
        "def calculate_variance(X):\n",
        "    mean = np.ones(np.shape(X)) * X.mean(0)\n",
        "    n_samples = np.shape(X)[0]\n",
        "    variance = (1 / n_samples) * np.diag((X - mean).T.dot(X - mean))\n",
        "\n",
        "    return variance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXuA4cP54uc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the covariance matrix for the dataset X\n",
        "def calculate_covariance_matrix(X, Y=np.empty((0,0))):\n",
        "    if not Y.any():\n",
        "        Y = X\n",
        "    n_samples = np.shape(X)[0]\n",
        "    covariance_matrix = (1 / (n_samples-1)) * (X - X.mean(0)).T.dot(Y - Y.mean(0))\n",
        "\n",
        "    return np.array(covariance_matrix, dtype=float)\n",
        " \n",
        "# Calculate the correlation matrix for the dataset X\n",
        "def calculate_correlation_matrix(X, Y=np.empty((0,0))):\n",
        "    if not Y.any():\n",
        "        Y = X\n",
        "    n_samples = np.shape(X)[0]\n",
        "    covariance = (1 / n_samples) * (X - X.mean(0)).T.dot(Y - Y.mean(0))\n",
        "    std_dev_X = np.expand_dims(calculate_std_dev(X), 1)\n",
        "    std_dev_y = np.expand_dims(calculate_std_dev(Y), 1)\n",
        "    correlation_matrix = np.divide(covariance, std_dev_X.dot(std_dev_y.T))\n",
        "\n",
        "    return np.array(correlation_matrix, dtype=float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paOfmJnTw-iM",
        "colab_type": "text"
      },
      "source": [
        "[Log regression objective function\n",
        "](https://towardsdatascience.com/introduction-to-logistic-regression-66248243c148)\n",
        "\n",
        "[Derivative of cost function](https://math.stackexchange.com/questions/477207/derivative-of-cost-function-for-logistic-regression)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmjD7NTmb9FJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression():\n",
        "  def __init__(self, n_iter, learning_rate):\n",
        "    self.n_iter = n_iter\n",
        "    self.learning_rate = learning_rate\n",
        "    \n",
        "  def fit(self, X, y):\n",
        "    num_features = np.shape(X)[1]\n",
        "    self.w = np.zeros(num_features)\n",
        "    label = self.predict(X)\n",
        "    for i in range(self.n_iter):\n",
        "      w_gradient = X.T.dot(X.dot(self.w) - y)\n",
        "      self.w -= self.learning_rate*X.T.dot(label - y)\n",
        "\n",
        "  \n",
        "  def predict(self, X):\n",
        "    y_pred = X.dot(self.w) \n",
        "    return np.round(sigmoid(y_pred)).astype(int)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kRh3R_AsjCA",
        "colab_type": "code",
        "outputId": "41e605c2-a4ba-4ab0-e54a-1063df261eac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "def main():\n",
        "  data = datasets.load_iris()\n",
        "  X, y = data.data, data.target\n",
        "  print(np.shape(X))\n",
        "  print(np.shape(y))\n",
        "  #X = normalize(X)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
        "  clf = LogisticRegression(4000, 0.01)\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  \n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  \n",
        "  print (\"Accuracy:\", accuracy)\n",
        "\n",
        "  # Reduce dimension to two using PCA and plot the results\n",
        "  pca = PCA()\n",
        "  pca.plot_in_2d(X_test, y_pred, title=\"Logistic Regression\", accuracy=accuracy)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150, 4)\n",
            "(150,)\n",
            "Accuracy: 0.16666666666666666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEjCAYAAADOsV1PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmYXGWZ9/HvLwlhERBJAgokNiio\niIoSQJSMQAQiqIDKkqCCiBl4ZUAcGREXkJlhEGfwFUUwIrK8LEbWIGhYBGRQlg57QCRCIAE0Yd+3\n5H7/OE9DpVPL6e6qOqeqf5/rqitV55w656nudN3n2e5HEYGZmVkeI4ougJmZdQ4HDTMzy81Bw8zM\ncnPQMDOz3Bw0zMwsNwcNMzPLzUHDuoKkvSVdPsj3zpW0TZOLVHqSfidpn6LLYZ1Fnqdh7SZpPrB/\nRFxZwLVPAxZGxHeGeJ4e4AHg+bTpMeDkiDh2KOc1K7tRRRfArMOtERGvSZoIXCtpTkRc0cwLSBoV\nEa8185xmg+XmKSsVSV+RNE/SE5JmSVqnYt8Oku6V9LSkn0m6VtL+ad++kv43PZekH0laJOkZSXdK\n2kTSdGBv4N8kPSfpknT8fEkfT89HSjpC0t8kPStpjqTxjcodEb3AXGDTivKuI+l8SYslPSDp4Ip9\nK0s6XdKTku6R9G+SFlbsny/pm5LuAJ6XNKrB+baQ1Js+7z8kHZ+2ryTp/0l6XNJTkm6WtHbad03F\nz2+EpO9IejD93M6Q9Oa0r0dSSNpH0kOSHpP07QH/cq0rOGhYaUjaDvgvYA/gbcCDwLlp31jgPOBb\nwBjgXuAjNU61A/BPwEbAm9P5Ho+IGcBZwHERsWpEfKrKe78OTAV2AlYH9gNeyFH2DwObAPPS6xHA\nJcDtwLrAZOBrknZMbzkS6AE2ALYHPl/ltFOBnYE1gKUNzvdj4McRsTrwDmBm2r5P+hmMJ/u5HQC8\nWOVa+6bHtqlMqwI/7XfM1sC70rW/J+k99X4m1p0cNKxM9gZOjYhbIuJlsgCxVeo/2AmYGxEXpKaa\nE4C/1zjPq8BqwLvJ+u3uiYhHc5Zhf+A7EXFvZG6PiMfrHP+YpBeBPwM/Ay5K2zcHxkXE0RHxSkTc\nD/wC2Cvt3wM4JiKejIiF6fP0d0JELIiIF3Oc71XgnZLGRsRzEXFDxfYxwDsjYklEzImIZ6pca2/g\n+Ii4PyKeI/vZ7yWpsgn7+xHxYkTcTha8PlDn52JdykHDymQdstoFAOnL63GyO+t1gAUV+wJY2P8E\nad8fyO6STwQWSZohafWcZRgP/G0AZR5Ldlf+r8A2wApp+9uBdVKT0FOSngKOANZO+5f5PP2eV9vW\n6HxfJqtZ/SU1QX0ybT8TmA2cK+kRScdJWoHlLfOzT89HVZwflg3SL6TPbcOMg4aVySNkX44ASHoT\n2V3yw8CjwHoV+1T5ur+IOCEiNgM2JvsyPaxvV4MyLCBr3skt3cEfD7wE/J+K8zwQEWtUPFaLiJ3S\n/mU+D1mwWu7U/cpV83wRcV9ETAXWAn4AnCfpTRHxakR8PyI2JmvO+yTwxSrXWuZnD0wAXgP+MYAf\nhQ0DDhpWlBVSJ23fYxRwDvAlSZtKWhE4BrgxIuYDlwLvk7RrOvarwFurnVjS5pK2THfUz5N9mS9N\nu/9B1mZfyynAv0vaMHWov1/SmJyf6ViyTvaVgJuAZ1Nn9sqpg30TSZunY2cC35L0FknrAgc1OHfd\n80n6vKRxEbEUeCq9Z6mkbSW9T9JI4Bmy5qqlVc5/DnCopPUlrUr2s/+1R21Zfw4aVpTLyDpk+x5H\npXkb3wXOJ7sTfwepzT4iHgN2B44ja7LaGOgFXq5y7tXJ2vufJGtmeRz4Ydr3S2Dj1MRzUZX3Hk/2\nhX452ZfsL4GVc36mS9M1vxIRS8ju6jclm8/xGFlAenM69miy5rUHgCvJOvmrfRYgq800ON8UYK6k\n58g6xfdKfSFvTed+BrgHuJasyaq/U9P2P6bzvwT8S87PbcOIJ/dZR0qjkxYCe0fE1UWXZ6gkHUj2\nRf+xostiVo9rGtYxJO0oaY3UdHUEIOCGBm8rJUlvk/TRND/iXWQd6RcWXS6zRjwj3DrJVsDZwGjg\nbmDX1ATTiUYDPwfWJ+uDOJdsyK5Zqbl5yszMcnPzlJmZ5eagYV0lDckNSe8uuixDJekgZXm4IqVR\nqdy3jaTblKV1v7bG+69Lx9yWJvZdlLZ/Nr3vur7hxJLeIenXrf9U1uncPGVdJX3xrQP8ISKObOF1\nRqZhsC0j6YNkQ3ivASamYcdIWgP4EzAlIh6StFZELGpwrvOBiyPiDEnXkKVl+Qzwloj4iaRzgO9F\nxH2t+0TWDVzTsK6RJqVtTZZSY69++76pLNvt7ZKOTdveKenKtO2WdLe9jaTfVrzvp5L2Tc/nS/qB\npFuA3ZVl5L05vf98Sauk49aWdGHafrukj0g6WtLXKs77n5IOqfd5IuLWNLGxv2nABRHxUDquUcBY\nHdiON/JiLQVWBFYBXpU0Cfi7A4bl4dFT1k12AX4fEX9Vlgp8s4iYI+kTad+WEfGCpDXT8WcBx0bE\nhWkW9wiqp/Oo9HhEfAhA0piI+EV6/h9kweonZMkHr42I3dJM7FXJ0nRcAPzfNMdkL2CL9N7bImLT\n5S9V00ZkM+qvIUvM+OOIOKPO8bsCV1UkKvwvsgmFj5Bl1/0N/YKsWS0OGtZNppLNhoZsCOtUYA7w\nceBXEfECQEQ8IWk1YN2IuDBtewlAUqNrVLb7b5KCxRpkgWF22r4dKb9TasJ6Gng6BbIPkiUBvLUv\ne+4AAwZkf7ebkaUoXxn4s6QbIuKvNY6fSjZ7nHS9K4ArACR9kWx2/kaSvkHWHHZI38/KrD8HDesK\nqfawHVl+qgBGAiHpsPrvXM5rLNtsu1K//c9XPD+NbK7I7akJa5sG5z6FbM2Kt5Kl7RishWQ1nufJ\nFmj6I1ma8uWCRupA3wLYrcq+VVJ5dgR+S9bH8TmyNOm/GEL5rIu5T8O6xeeAMyPi7RHRExHjyXIo\nTSK7q/5SRZ/DmhHxLLBQ0q5p24pp/4NkualWTB3Ok+tcczXgUWWJEfeu2H4VcGA670ilFfDIZnxP\nIVsbYzaDdzGwtbLV/FYBtiTLK1XN54Df9tWk+jmMbM2OV8lqLEHW37HKEMpmXc5Bw7rFVJZPw3E+\nMDUifg/MAnol3QZ8I+3/AnCwsiVV/wS8NSIWkCUsvCv9e2uda34XuBG4HvhLxfZDgG0l3UnWPLYx\nQES8AlwNzKwceZXKtBxJBytbAnY94A5Jp6Tz3AP8HriDLPvtKRFxV3rPZapYIpesr+KcKudeB9gi\nIvo6x38C3Ey2st/ZdT6zDXMecmvWJqkD/BZgd49Usk7lmoZZG0jamGz98KscMKyTuaZhZma5uaZh\nZma5OWiYmVluhc7TkHQq2RKWiyJikyr79wa+SbbYzrPAgRFxe71zjh07Nnp6elpQWjOz7jVnzpzH\nImJco+OKntx3GvBToFYKhAeAj0XEkykVxAyyMek19fT00Nvb29RCmpl1O0kP5jmu0KAREX+U1FNn\n/58qXt5ANl7dzMwK0kl9Gl8Gfld0IczMhrOim6dykbQtWdDYusb+6cB0gAkTJrSxZGZmw0vpaxqS\n3k+W6G2Xvqyg/UXEjIiYGBETx41r2I9jZmaDVOqgIWkC2RoEX6iT9tnMzNqk6CG355Clkx6bErMd\nCawAEBEnA98DxgA/S+scvBYRE4sprZmZFT16amqD/fsD+7epOGZm1kCpm6esc1x19nXs3XMgO4zc\ng717DuSqs68rukhm1gIdMXrKyu2qs6/jR9NP5uUXXgFg0UOP8aPpJwMwedqkIotmZk3mmoYN2alH\nnP16wOjz8guvcOoRXsvHrNs4aNiQLV5QdSR0ze1m1rkcNGzIxo0fM6DtZta5HDRsyPY7ZhorrjJ6\nmW0rrjKa/Y6ZVlCJzKxV3BFuQ9bX2X3qEWezeMHjjBs/hv2OmeZOcLMu1HXLvU6cODGcGt3MbGAk\nzckzedrNU2ZmlpuDhpmZ5eag0QSeDW1mw4U7wofIs6HNbDhxTWOIPBvazIYTB40h6pTZ0G5CM7Nm\ncNAYok6YDd3XhLbooceIiNeb0Bw4zGygHDSGqBNmQ7sJzcyaxR3hQ9QJs6E7pQnNzMrPQaMJJk+b\nVKog0d+48WNY9NBjVbebmQ2Em6e6UP9O7y133qz0TWhm1hkcNLpMtU7vy0+/mh322Za1JoxFEmtN\nGMuhMw4ode3I2s8j7CwPN091mVqd3jdeOoez5p9UUKms7DxJ1fIqtKYh6VRJiyTdVWO/JJ0gaZ6k\nOyR9qN1l7DTu9LbBKNsIO9d6yqvo5qnTgCl19n8C2DA9pgO+VW6gE+aNWPmU6WbD84rKrdCgERF/\nBJ6oc8guwBmRuQFYQ9Lb2lO6ztQJ80asfMp0s1G2Wo8tq+iaRiPrAgsqXi9M25YhabqkXkm9ixcv\nblvhymjytEkcOuMAd3rbgJTpZqNMtR5bXld0hEfEDGAGZCv3FVycwpV93oiVT5kmqXpeUbmVPWg8\nDIyveL1e2mZmTVaWm439jpm2zEgucBNrmZS9eWoW8MU0iurDwNMR8WjRhTKz1nETa7kVWtOQdA6w\nDTBW0kLgSGAFgIg4GbgM2AmYB7wAfKmYkppZO5Wl1mPLKzRoRMTUBvsD+GqbimNmZg2UvXnKzMxK\nxEHDzMxyc9AwM7PcHDTMzCw3Bw0zM8vNQcPMzHJz0BiAbk/X3O2frxv5d2btVvY0IqXR7YvUdPvn\n60b+nVkRXNOootrd20DSNXfi3Z/TUXce/86GnzJ8t7im0U+tu7f+f5x9+qdr7tS7P6ej7jz+nQ0v\nZflucU2jn1p3byNGVv9R9U/X3Kl3f2VahKdSGe6syqqsvzNrjbJ8tzho9FPrLm3pkqW5Fqnp1Lu/\nMi3C08fLftZXxt+ZtU5ZvlscNPqpdZfWl565UbrmTr37K2M66rLcWQ1Vq2pLZfydWeuU5btFWSLZ\n7jFx4sTo7e0d9Pv7txtCdveW949xqO+3N+wwcg+q/f+UxOVLZhZQooHz/wdrllb/X5I0JyImNjrO\nNY1+hnr31oy7P7fjZ8pyZzUU3VJbsuKVpWbZsKYhaYWIeLXftrERsfwiviUw1JpG0Xxn+oYy/yz6\nhmE3Wk+7G2pLVm55/y82MuSahqRt02p6j0q6XFJPxe7LB1wiy8V3pm8oy51VfwPpoO+G2pKVVxGD\nReo1Tx0H7BgRY4EZwBVpnW4AtaxEw1xZRkiUxeRpkzhr/klcvmQmZ80/qfCAAQML7B7hZK1UxE1m\nvaAxOiLmAkTEecCuwOmSdgW6q/e8RHxnWn4DCexF1pbcN9b9irjJrDcj/FVJb42IvwNExFxJk4Hf\nAu9oWYmGuf2OmVa1Hd93puUxbvwYFj20fJdercA+edqktteQyjJ72FproP8Xm6FeTeNwYO3KDRGx\nEPgYcGzLSjTMlbUd397QCU1O7hsbHor4v1joPA1JU4AfAyOBUyLi2H77JwCnA2ukYw6PiMvqnbPT\nR09ZZ2jWiJVWKeuorbL/3DpRu0dPFRY0JI0E/gpsDywEbgamRsTdFcfMAG6NiJMkbQxcFhE99c7r\noGEGe/ccWLXZYq0JYzlr/kkFlKj6EGoEBIwYOYKlS5ay1oSxDiQF6YTJfVsA8yLi/oh4BTgX2KXf\nMQGsnp6/GXikjeUz61hlbEKr1mTWN6Rm6ZKlAM4v1gEaBg1Ju+fZNgjrAgsqXi9M2yodBXw+zRe5\nDPiXGmWcLqlXUu/ixYubUDSzzlbGvrG8I3rc91JuedbT+BbwmxzbWmEqcFpE/I+krYAzJW0SEUsr\nD4qIGWRzSZg4caKHA5tRzKitemqN9KlmuM5L6gQ1g4akTwA7AetKOqFi1+rAa0249sPA+IrX66Vt\nlb4MTAGIiD9LWgkYCyxqwvXNrI2qDSevxfOSyqte89QjQC/wEjCn4jEL2LEJ174Z2FDS+pJGA3ul\nc1d6CJgMIOk9wEqA25/MOlBlkxlQO6+EYMudN2tbuWxgatY0IuJ24HZJZ/dPWNgMEfGapIOA2WTD\naU9NEwiPBnojYhbwr8AvJB1K1mW2b3RbLnezYUgSq625KhHBs088t+zOgMtPv5r3fvRdpWpes0ye\nLLcfJeuQfjtZkBEQEbFBy0s3CB5ya1ZOtbIWj15p9PKBg2KHBw9HeYfc5ukI/yVwKFnT1JKhFszM\nhqdas9Rr9XG4M7yc8gSNpyPidy0viZl1tYEGAXeGl1OeyX1XS/qhpK0kfajv0fKSmVlXqRUEVh+z\nWukmIlpteWoaW6Z/K9u6Atiu+cUxs25VK4Pz//nxlwCck6pDNAwaEbFtOwpiZt2tLwjUCg4OEp0h\nz+iptYFjgHUi4hMpceBWEfHLdhRwoDx6ysxs4JqZsPA0srkU66TXfwW+NviimZlZp8oTNMZGxExg\nKWST8vDQ22HFy4aaWZ88QeN5SWNISYwlfRh4uqWlstLom5C16KHHiAinrm4SB2LrVHmCxtfJckK9\nQ9L1wBnUSFFu3cfLhjafA7F1soZBIyJuIVsX/CPAPwPvjYg7Wl0wK4daE7I8W3fwHIitk+WZpwHZ\nKns96fgPSSIizmhZqaw0aq2B4Nm6A1O5jnOtEYsOxNYJ8qzcdybw38DWwObp0XBYlnWHMi4b2mn6\nN0fV4kBsnSBPTWMisLFTkg9PjSZkWWNV18bux4HYOkWeoHEX8Fbg0RaXxUqqbMuGdpp6zU6SHIit\no+QJGmOBuyXdBLzctzEiPt2yUpk1SWVfQlFfzrX6hbxehHWiPEHjqFYXwqwV+i/60ze0Fdqb56hW\noj43R1knyjPk9lrgL8Bq6XFP2mZWakMZ2trMyXeVa2NLYq0JYzl0xgFujrKO1LCmIWkP4IfANWRL\nvf5E0mERcV6Ly2Y2JIOdY9KKGor7haxb5JkR/m1g84jYJyK+SDZn47utLZbZ0NUawtpoaGvRk++c\nYsTKLE/QGBERiypeP57zfR3Ff6jl0ozfx2DnmBQ5C94pRqzs8nz5/17SbEn7StoXuBS4rBkXlzRF\n0r2S5kk6vMYxe0i6W9JcSS251fMfark06/cx2L6EwdZQmqHoWo5ZIw0XYQKQ9BmyGeEA10XEhUO+\nsDSSbG2O7YGFwM3A1Ii4u+KYDYGZwHYR8aSktfrVepYzmEWY9u450EMiS6To30f/Pg3Iaijt6Lze\nYeQeVWeNS+LyJTNbem0b3vIuwpQ399SfyNbQWEr25d4MWwDzIuJ+AEnnArsAd1cc8xXgxIh4EqBR\nwBgsJ+Url6J/H0XOgneuLyu7PLmn9gduAnYDPgfcIGm/Jlx7XWBBxeuFaVuljYCNJF0v6QZJU2qU\ncbqkXkm9ixcvHnBBimyOsOWV4fcxedokzpp/EpcvmclZ809q28gn5/qyssvTp3EY8MGI2Dci9gE2\nA77Z2mK9bhSwIbANMBX4haQ1+h8UETMiYmJETBw3btyAL+I/1HKp9vtAsOXOmxVToDbynA4ruzzN\nU48Dz1a8fjZtG6qHgfEVr9dL2yotBG6MiFeBByT9lSyINKuJDHBSvrKZPG0Sc6+/l0tOnp3WiwQC\nLj/9at770Xd1/e/FczqszBp2hEs6A3gfcDHZn/AuwB3pQUQcP6gLS6PIOsInkwWLm4FpETG34pgp\nZJ3j+0gaC9wKbBoRNYPWYDrCrXyK7gw3G27ydoTnaZ76G3ARb9zzXQw8wBtpRQYlIl4DDgJmA/cA\nMyNirqSjJfUlQ5wNPC7pbuBq4LB6AcO6R7M7wz0Px6w5cg257SSuaXSHZtY0qg2hBVhtzVX56gn7\nuSnIjCbWNCRNlHShpFsk3dH3aE4xzapr5uCEWosgPfvEc57EaTZAeTrCzyIbQXUn2TwNs5Zr5uCE\nek1afbOtXdswyydP0FgcEbNaXhKzfpo1iqjWhLk+nsRpll+ejvAjJZ0iaaqkz/Q9Wl4ysyapOu+j\nwrjxY9xRbpZTnprGl4B3AyvwRvNUABe0qlBmzdRXW/nZIb/imcefXWbfiquMZsudNyvFCn9mnSDP\nPI17I+JdbSrPkHn0lNVTbc3wU48423NCbNhrZsLCP0nauDL7rFmnqtZP8oMv/KTqse7rMFtenj6N\nDwO3pXUv7pB0p4fcWlk0oy+iDAkSzTpFnqAxhSzf0w7Ap4BPpn/NCtWsxZrKkLDSHfHWKRoGjYh4\nEFiDLFB8ClgjbTMrVLNWuSs6s6xXjrROkqcj/BCyxZD6RkvtBsyIiOoNwQVzR/jw0S2r3Dk5o5VB\nMxMWfhnYMiK+FxHfI+vj+MpQC2g2VK3oiyiimajolQrNBiJP0BDZUq99lqRtZoVqdl9EUc1E7oi3\nTpInaPwKuFHSUZKOAm4AftnSUpnl0Oy+iGb1kQxUGTrizfJqOE8jIo6XdA2wddr0pYi4taWlMsup\nmavcFdVM5JUjrZPUDBqSNgfGRsTvIuIW4Ja0fSdJIyJiTrsKadYOtRIbtqOZKE/wqzab3YHF2q1e\n89QPgGqzwOcCP2xNccyKU+ZmIg/LtbKoFzRWqzYfI20b27oimRWj6Pka9RTV32LWX70+jbfU2bdK\nswtiVgbN7CNpJg/LtbKoV9O4UtJ/Snp9eK0yRwN/aH3RzKyPh+VaWdQLGv8KbADMk3S+pPOB+4CN\ngK834+KSpqREiPMkHV7nuM9KCkkNZyva8NatOZzK3N9iw0vN5qmIeB6YKmkD4L1p89yIuL8ZF5Y0\nEjgR2B5YCNwsaVb/FOySVgMOAW5sxnWtHFoxEqivs7hyMaVjv3ACc6+/l4NP3L8ZxS6Mh+VaWeSZ\np3E/0JRA0c8WwLy+ICTpXGAXlh+x9e9kI7kOa0EZrMnyBINqX+7NWCmvWmcxAZecPJv3fvRdHf8F\nW9b+Fhte8swIb5V1gQUVrxemba+T9CFgfERc2s6C2eDkHRbaqpFANTuFA48yMmuSIoNGXZJGAMeT\n9a00Ona6pF5JvYsXL2594ayqvMGgVSOB6nUKe5SRWXPUDBqS1qz3aMK1HwbGV7xeL23rsxqwCXCN\npPlk2XVnVesMj4gZETExIiaOGzeuCUWzwcgbDFo1Emi/Y6bVTKXpUUZmzVGvpjEH6E3/9n80Y8GK\nm4ENJa0vaTSwFzCrb2dEPB0RYyOiJyJ6yBIlfjoivFhGSeUNBq0aCTR52iQ+dcCOywUOjzIya56a\nQSMi1o+IDdK//R8bDPXCEfEacBAwG7gHmBkRcyUdLenTQz2/tV/eYNDKmdcHn7g/h595cClndZt1\ng4Yr9wFIegvZOuEr9W2LiD+2sFyD5pX7iuWkegPnn5mVQd6V+/Is97o/2TyJ9YDbyPoW/hwR2zWj\noM3moGGdpP/wY8hqZ64dWbs1c7nXQ4DNgQcjYlvgg8BTQyyfmeFEhNZ58gSNlyLiJQBJK0bEX4B3\ntbZYZgPTqelDnIjQOk3DGeHAQklrABcBV0h6ElguZbpZUVo1w7wdilz4yWwwGtY0ImK3iHgqIo4C\nvku2PviurS6YWV6d3MRTb8TZCV89hR1X2JPtR+zOjivsyQlfPaWgUpq9IdeMcEkfknQw8H5gYUS8\n0ug91v3K0iTUyU08tYYfz73+Xi45aTZLlywFYOmSpVxy0mwHDitcntFT3wN2By5Im3YFfhMR/9Hi\nsg2KR0+1R5lG/ezdc2DVJp61JozlrPkntbUszbLjCnu+HjAqjRg5gtmv/rqAElm3a+boqb2BzSPi\nyIg4kmzI7ReGWkDrbK1oEhpszaUb15qoFjDqbTdrlzwd4Y+QTep7Kb1ekWVzRNkw1OwmoaF0Znfj\nWhMjRo6oWdMwK1KeoPE0MFfSFUCQLZp0k6QTACLi4BaWz0qq2aN+6tVc8nz5d9taEztP355LTppd\ndbtZkfIEjQvTo881rSmKdZL9jplWtU9jsE1CndyZ3Qp9Kw1eOuMKli5ZyoiRI9h5+vYdvwKhdb48\nK/ed3o6CWGdpdpOQ5yss7+AT93eQsNKpGTQkzYyIPSTdSdYstYyIeH9LS2al18wmoWbXXMysNerV\nNA5J/36yHQWx4a0bO7PbwRlyrd3yzNNYH3i0Iv/UysDaETG/9cUbOM/TsOGiTHNlrPM1c57Gb4DK\nsX9L0jazISvLrPJO1MnpU6xz5Rk9NaoybUhEvJKWZzUbkk5ONFgGHnFmRchT01hcufyqpF2A5Ye5\nWFt1wx2675SHJu+a7GbNlCdoHAAcIekhSQuAbwL/3NpiWT19d+iLHnqMiHj9Dr3TAofvlIemG9On\nWPnlSY3+t4j4MLAx8J6I+EhEzGt90ayWbrlD953y0NTKkOumPWulhn0aklYEPgv0AKMkARARR7e0\nZFZTt9yhe27G0HVb+hQrvzzNUxcDuwCvAc9XPIZM0hRJ90qaJ+nwKvu/LuluSXdIukrS25tx3U7X\nLXfovlM26zx5Rk+tFxFTmn1hSSOBE8kSIC4EbpY0KyLurjjsVmBiRLwg6UDgOGDPZpel03TTHbrv\nlM06S56axp8kva8F194CmBcR96chveeS1WheFxFXR8QL6eUNwHotKEfH8R26mRUlT01ja2BfSQ8A\nLwMCogm5p9YFFlS8XghsWef4LwO/G+I1u4bv0M2sCHmCxidaXooGJH0emAh8rMb+6cB0gAkTJrSx\nZGZmw0vN5ilJq6enz9Z4DNXDwPiK1+tRZUVASR8Hvg18OiJernaiiJgRERMjYuK4ceOaUDQzM6um\nXk3jbLIMt3PIUqOrYl8AGwzx2jcDG6aEiA8DewHL9ORK+iDwc2BKRCwa4vXMzGyIatY0IuKTyiZl\nfCwiNoiI9SseQw0YRMRrwEHAbOAeYGZEzJV0dEXakh8CqwK/kXSbpFlDva4NT92QdsWsDPKkRr8z\nIloxeqolnBrd+nMKcbPGmpka/RZJmzehTGaF6Ja0K2ZlkGf01JbA5yXNJ5sJ3qwht2Zt0S1pV8zK\nIE/Q2LHlpTBroXHjx7DooeWz+Xda2hWzMqg35HYlSV8DDgOmAA9HxIN9j7aV0GyIhlsKcXf6WyvV\nq2mcDrwKXEc2wW9j4JB2FMqsmfo6u0894mwWL3iccePHsN8x07qyE9yrIVqr1Rw9VTlqStIo4KaI\n+FA7CzcYHj1lw9nePQdWbYosjcVFAAALrElEQVRba8JYzpp/UgElsk7RjNFTr/Y9SXMqzKzkurnT\n381u5VAvaHxA0jPp8Szw/r7nkp5pVwGtc/mPvP26Za2V/rplieNuUG9G+MiIWD09VouIURXPV6/1\nPjPwH3lRurXT33NtyiPP5D6zAfMfeTG6da2Vbm526zR55mmYDZj/yIvTjWuteK5NebimYS3RrW3r\nVoxubXbrRA4a1hL+I7dm6tZmt07UMMttp/E8jfK46uzrhsWEOrNukHeehoOGmZk1NTW6mZkZ4KBh\nZmYD4KBhZma5OWiYmVluDhpmZpabg4aZmeVWaNCQNEXSvZLmSTq8yv4VJf067b9RUk/7S2lmZn0K\nCxqSRgIn8saqgFMlbdzvsC8DT0bEO4EfAT9obynNzKxSkTWNLYB5EXF/RLwCnAvs0u+YXciWnQU4\nD5gsSW0so5mZVSgyaKwLLKh4vTBtq3pMWj3waWC5jHeSpkvqldS7ePHiFhXXzMy6oiM8ImZExMSI\nmDhu3Liii2Ml5ZUEzYauyPU0HgbGV7xeL22rdsxCSaOANwNekMEGrG8lwb6FofpWEgScRNFsAIqs\nadwMbChpfUmjgb2AWf2OmQXsk55/DvhDdFuGRWsLryRo1hyF1TQi4jVJBwGzgZHAqRExV9LRQG9E\nzAJ+CZwpaR7wBFlgMRswryRo1hyFLvcaEZcBl/Xb9r2K5y8Bu7e7XNZ9vFyoWXN0RUe4WSNeSdCs\nOQqtaZi1S19nt1cSNBsar9xnHcVLyJq1Rt6V+1zTsI7hYbNmxXOfhnUMD5s1K56DhnUMD5s1K56D\nhnWMWsNjO23YrNOZWCdz0LCO0Q3DZvv6ZRY99BgR8Xq/jAOHdQoHDesYk6dN4tAZB7DWhLFIYq0J\nYzl0xgEd1QnufhnrdB49ZR1l8rRJHRUk+nO/jHU61zTM2qhb+mVs+HLQMGujbuiXseHNzVNmbeR0\nJtbpnEbEzMxypxFx85SZmeXmoGEt58lsZt3DfRrWUk4yaNZdXNOwlvJkNrPu4qBhLeXJbGbdxUHD\nWsqT2cy6i4OGtZQns5l1l0KChqQ1JV0h6b7071uqHLOppD9LmivpDkl7FlFWG5puSDJoZm8oZHKf\npOOAJyLiWEmHA2+JiG/2O2YjICLiPknrAHOA90TEU/XO7cl9ZmYDV/bJfbsAp6fnpwO79j8gIv4a\nEfel548Ai4BxbSuhmZktp6igsXZEPJqe/x1Yu97BkrYARgN/q7F/uqReSb2LFy9ubknNzOx1LZvc\nJ+lK4K1Vdn278kVEhKSabWSS3gacCewTEUurHRMRM4AZkDVPDbrQZmZWV8uCRkR8vNY+Sf+Q9LaI\neDQFhUU1jlsduBT4dkTc0KKimplZTkU1T80C9knP9wEu7n+ApNHAhcAZEXFeG8tmZmY1FBU0jgW2\nl3Qf8PH0GkkTJZ2SjtkD+CdgX0m3pcemxRTXzMzA62mYmRn5h9x2XdCQtBh4sN/mscBjBRSnLIbz\n5/dnH76G8+cfzGd/e0Q0nNbQdUGjGkm9eSJotxrOn9+ffXh+dhjen7+Vn925p8zMLDcHDTMzy224\nBI0ZRRegYMP58/uzD1/D+fO37LMPiz4NMzNrjuFS0zAzsybo6qAhafe0HsdSSRMrto+RdLWk5yT9\ntMgytkqtz572fUvSPEn3StqxqDK2i6QPpLVZ7pR0SUpPMyykdWluSJNje1Pyz2FB0q8rJgbPl3Rb\n0WVqN0n/Iukv6bvguGacs2W5p0riLuAzwM/7bX8J+C6wSXp0o6qfXdLGwF7Ae4F1gCslbRQRS9pf\nxLY5BfhGRFwraT/gMLLf/3BwHPD9iPidpJ3S622KLVJ7RMTrC7dJ+h/g6QKL03aStiVbhuIDEfGy\npLWacd6urmlExD0RcW+V7c9HxP+SBY+uVOuzk/0nOjciXo6IB4B5QLfffW4E/DE9vwL4bIFlabcA\n+mpWbwYeKbAshZAksrRE5xRdljY7EDg2Il4GiIiqiWEHqquDhlW1LrCg4vXCtK2bzSULlgC7A+ML\nLEu7fQ34oaQFwH8D3yq4PEWYBPyjb1G3YWQjYJKkGyVdK2nzZpy045un6q3bERHLZc/tJsP5s/fX\nYP2W/YATJH2XLMPyK+0sW6s1+OyTgUMj4nxJewC/JEsS2hVy/g1MpUtrGQ1+96OANYEPA5sDMyVt\nEEMcMtvxQaPeuh3dbpCf/WGWvdNeL23raDl+FjvA62vP79z6ErVPg7VrzgAOSS9/Q9a/0zUa/d4l\njSLr29usPSVqrwa/+wOBC1KQuEnSUrKcVENa3tTNU8PPLGAvSStKWh/YELip4DK1VF8HoKQRwHeA\nk4stUVs9AnwsPd8OGG5NNB8H/hIRC4suSAEuAraF12+WRtOEBI4dX9OoR9JuwE+AccClkm6LiB3T\nvvlkHYSjJe0K7BARdxdW2Car9dkjYq6kmcDdwGvAV7t85BTAVElfTc8vAH5VZGHa7CvAj9Md90vA\n9ILL02570aVNUzmcCpwq6S6yJtl9hto0BZ4RbmZmA+DmKTMzy81Bw8zMcnPQMDOz3Bw0zMwsNwcN\nMzPLzUHDSkXSkpSV9C5Jv5G0So3jLpO0xiDOv46k84ZQvvmSxlbZvqqkn0v6m6Q5kq6RtOVgr1MG\nKUPuTjX2dX2maKvOQcPK5sWI2DQiNiEbW35A5U5lRkTEThHx1EBPHhGPRMTnmlXYCqcATwAbRsRm\nwJfIZt92sk2BqkGDNzJFf6N9xbEycNCwMrsOeKeknrT2xxlkKd/H993xp333SPpFWjPgckkrA0h6\np6QrJd0u6RZJ70jH35X27yvp4lQruE/SkX0XlnRRqjHMlVR3QpykdwBbAt+JiKUAEfFARFya9n89\n1ZzukvS1tK0nrXNwmqS/SjpL0sclXZ/KskU67ihJZypbD+Q+SV9J2yXph+mcd0raM23fJn2e89L5\nz0pZXpG0WUpcN0fSbElvS9uvkfQDSTelskySNBo4Gtgz1fz2rPzMwyFTtNUQEX74UZoH8Fz6dxRw\nMVl65x5gKfDhiuPmk93J95DNbN80bZ8JfD49vxHYLT1fCVglHX9X2rYv8CgwBliZLCBNTPvWTP/2\nbR9Ted1+Zf40cGGNz7MZcCfwJmBVsoy7H6wo9/vIbt7mkM3gFVlG3ovS+48Cbk/lGEuWoXgdsvTu\nVwAjgbWBh4C3ka2V8TRZTrERwJ+BrYEVgD8B49J59wROTc+vAf4nPd8JuLLi5/PTBr+vhsf40V2P\nrk4jYh1pZb2xwtp1ZFlZ1wEejIgbarzngYjoe88coEfSasC6EXEhQES8BJBuuitdERGPp30XkH3B\n9gIHp1QskCV43BB4fBCfZ2uygPJ8xTUmkeUAeyAi7kzb5wJXRURIupMsqPS5OCJeBF6UdDXZ+idb\nA+dElgLmH5KuJctk+gxwU6RcS+ln2QM8Rbbg2BXpZzCSLGD2uSD9O6fftc2W4aBhZfNiRGxauSF9\nyT1f5z0vVzxfQnZXnlf/PDohaRuyRHdbRcQLkq4hq6nUMhf4gKSRMbA8XpXlXlrxeinL/m0uV8YB\nnHdJOpeAuRGxVYP39B1vVpX7NKwrRcSzwMKUjBJlWX2rjcTaXtKaqR9kV+B6shXunkwB491k6xHU\nu9bfyGon36/oP+iRtDNZbWlXSatIehOwW9o2ELtIWknSGLLmp5vTOfaUNFLSOOCfqJ+t+F5gnKSt\nUvlWkPTeBtd9FlhtgGW1LuegYd3sC2TNTHeQtedXW6zmJuB84A7g/IjoBX4PjJJ0D3AsUKtZrNL+\nZH0L81JH+2nAooi4JT2/iayP5ZSIuHWAn+MO4OpUjn+PiEeAC9P224E/AP8WEX+vdYKIeAX4HPAD\nSbcDtwEfaXDdq4GNq3WEw+uZoo8H9pW0UNn689blnOXWhi1J+5J1fB9UdFlqkXQU2eCA/y66LGbg\nmoaZmQ2AaxpmZpabaxpmZpabg4aZmeXmoGFmZrk5aJiZWW4OGmZmlpuDhpmZ5fb/AUHVmQcJouAY\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meYxg6gN4YWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import sys\n",
        "import os\n",
        "from sklearn import datasets\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cmx\n",
        "import matplotlib.colors as colors\n",
        "import numpy as np\n",
        "\n",
        "class PCA():\n",
        "    \"\"\"A method for doing dimensionality reduction by transforming the feature\n",
        "    space to a lower dimensionality, removing correlation between features and \n",
        "    maximizing the variance along each feature axis. This class is also used throughout\n",
        "    the project to plot data.\n",
        "    \"\"\"\n",
        "    def __init__(self): pass\n",
        "\n",
        "    # Fit the dataset to the number of principal components\n",
        "    # specified in the constructor and return the transformed dataset\n",
        "    def transform(self, X, n_components):\n",
        "        covariance = calculate_covariance_matrix(X)\n",
        "\n",
        "        # Get the eigenvalues and eigenvectors.\n",
        "        # (eigenvector[:,0] corresponds to eigenvalue[0])\n",
        "        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n",
        "\n",
        "        # Sort the eigenvalues and corresponding eigenvectors from largest\n",
        "        # to smallest eigenvalue and select the first n_components\n",
        "        idx = eigenvalues.argsort()[::-1]\n",
        "        eigenvalues = eigenvalues[idx][:n_components]\n",
        "        eigenvectors = np.atleast_1d(eigenvectors[:, idx])[:, :n_components]\n",
        "\n",
        "        # Project the data onto principal components\n",
        "        X_transformed = X.dot(eigenvectors)\n",
        "\n",
        "        return X_transformed\n",
        "\n",
        "    def get_color_map(self, N):\n",
        "        color_norm  = colors.Normalize(vmin=0, vmax=N-1)\n",
        "        scalar_map = cmx.ScalarMappable(norm=color_norm, cmap='hsv') \n",
        "        def map_index_to_rgb_color(index):\n",
        "            return scalar_map.to_rgba(index)\n",
        "        return map_index_to_rgb_color\n",
        "\n",
        "    # Plot the dataset X and the corresponding labels y in 2D using PCA.\n",
        "    def plot_in_2d(self, X, y=None, title=None, accuracy=None, legend_labels=None):\n",
        "        X_transformed = self.transform(X, n_components=2)\n",
        "        x1 = X_transformed[:, 0]\n",
        "        x2 = X_transformed[:, 1]\n",
        "        class_distr = []\n",
        "\n",
        "        y = np.array(y).astype(int)\n",
        "\n",
        "        # Color map\n",
        "        cmap = plt.get_cmap('viridis')\n",
        "        colors = [cmap(i) for i in np.linspace(0, 1, len(np.unique(y)))]\n",
        "\n",
        "        # Plot the different class distributions\n",
        "        for i, l in enumerate(np.unique(y)):\n",
        "            _x1 = x1[y == l]\n",
        "            _x2 = x2[y == l]\n",
        "            _y = y[y == l]\n",
        "            class_distr.append(plt.scatter(_x1, _x2, color=colors[i]))\n",
        "\n",
        "        # Plot legend\n",
        "        if not legend_labels is None: \n",
        "            plt.legend(class_distr, legend_labels, loc=1)\n",
        "\n",
        "        # Plot title\n",
        "        if title:\n",
        "            if accuracy:\n",
        "                percent = 100 * accuracy\n",
        "                plt.suptitle(title)\n",
        "                plt.title(\"Accuracy: %.1f%%\" % percent, fontsize=10)\n",
        "            else:\n",
        "                plt.title(title)\n",
        "\n",
        "        # Axis labels\n",
        "        plt.xlabel('Principal Component 1')\n",
        "        plt.ylabel('Principal Component 2')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    # Plot the dataset X and the corresponding labels y in 3D using PCA.\n",
        "    def plot_in_3d(self, X, y=None):\n",
        "        X_transformed = self.transform(X, n_components=3)\n",
        "        x1 = X_transformed[:, 0]\n",
        "        x2 = X_transformed[:, 1]\n",
        "        x3 = X_transformed[:, 2]\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        ax.scatter(x1, x2, x3, c=y)\n",
        "        plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}